# -*- coding: utf-8 -*-
"""FedKL_test.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ykisJEd2ZyAxeG23yS9zemnGb253G0VE
"""

import math
import json
import multiprocessing
import numpy as np
import matplotlib.pyplot as plt
from itertools import product, repeat
from tqdm import tqdm

from util.cspolicy import client_sample_uni, client_sample_KL, client_sample_KLGap
from util.metric import accuracy, crossentropy
from util.misc import softmax, mod_softmax

np.random.seed(seed=12345)

# train_data = {}
# test_data = {}
grid = []

N = 10
m = 3
d = 28 * 28
K = 10
B = 100
I = 5
T = 500

M = 50
C = 1 - (K * math.exp(-M))
L = math.sqrt(2)  # TODO: Need to calculate
mu = 1e-6
gamma = (4 * L) / mu


def import_data():
    train_path = "data\FMNIST\FMNIST_alpha005_train.json"
    test_path = "data\FMNIST\FMNIST_alpha005_test.json"
    with open(train_path, "r") as f:
        train_data = json.load(f)
    with open(test_path, "r") as f:
        test_data = json.load(f)
    return train_data, test_data


def generate_grid():
    cartesian = product(np.linspace(0, 1, 6), repeat=N)
    grid = [
        point for point in cartesian if sum(point) == 1 and np.count_nonzero(point) >= m
    ]
    return grid


def init():
    global train_data
    global test_data
    global grid
    train_data, test_data = import_data()
    grid = generate_grid()


def gradient_eval(w, data, C, M):  # TODO: Check gradient correctness
    sz = len(data["y"])
    batch_idx = np.random.choice(sz, B, replace=False)

    gradient = np.zeros((K, d))
    for i in range(K):
        for n in batch_idx:
            x_n = np.array(data["x"][n])
            y_n = np.array(data["y"][n])

            pred = [np.dot(w[k], x_n) for k in range(K)]
            pclass = softmax(pred)
            mod_pclass = mod_softmax(pred, C, M)

            A1 = 0
            for k in range(K):
                y_nk = int(y_n == k)
                A1 += y_nk * (pclass[k] / mod_pclass[k])
            A2 = int(y_n == i) / mod_pclass[i]
            gradient[i] += (x_n * pclass[i]) * (A1 - A2)

        gradient[i] = C * gradient[i] + mu * w[i]
    return gradient / B


def FedAvg(policy):
    w_avg = np.random.rand(K, d)
    loss_train = np.zeros(T)
    loss_test = np.zeros(T)
    client_cnt = np.zeros(N)

    user_name = ["f_{0:05d}".format(n) for n in range(N)]
    train_dist = np.array(
        [train_data["distribution"][uname] for uname in train_data["distribution"]]
    )
    test_dist = np.array(
        [test_data["distribution"][uname] for uname in test_data["distribution"]]
    )
    test_dist = sum(test_dist) / N

    for t in tqdm(range(T)):
        eta = (3 / (I * mu)) / (t + gamma)
        w_sum = np.zeros((K, d))

        client_loss_train = [
            crossentropy(w_avg, train_data["user_data"][user_name[n]], B, K, C, M, mu)
            for n in range(N)
        ]
        client_loss_test = [
            crossentropy(w_avg, test_data["user_data"][user_name[n]], B, K, C, M, mu)
            for n in range(N)
        ]

        loss_train[t] = sum(client_loss_train) / N
        loss_test[t] = sum(client_loss_test) / N

        if policy == "uniform":
            participants_set = client_sample_uni(N, m)
        elif policy == "KL":
            participants_set = client_sample_KL(train_dist, test_dist, N, m)
        elif policy == "KLGap":
            participants_set = client_sample_KLGap(
                train_dist, test_dist, client_loss_train, N, m, M, grid
            )

        for n in participants_set:  # TODO: parallel
            client_cnt[n] += 1
            w_n = w_avg[:]
            for i in range(I):
                gradient = gradient_eval(
                    w_n, train_data["user_data"][user_name[n]], C, M
                )
                w_n = w_n - (eta * gradient)
            w_sum = w_sum + w_n
        w_avg = w_sum / m

    acc_train = [
        accuracy(w_avg, train_data["user_data"][user_name[n]], K, C, M)
        for n in range(N)
    ]
    acc_test = [
        accuracy(w_avg, test_data["user_data"][user_name[n]], K, C, M) for n in range(N)
    ]

    return loss_train, loss_test, sum(acc_train) / N, sum(acc_test) / N, client_cnt / T


def main():
    # Init

    pool = multiprocessing.Pool(initializer=init, processes=1)
    policy_list = ["uniform"]
    results = pool.map(FedAvg, policy_list)

    # TODO: Implement the parallel using multiprocessing module

    loss_train_uni, loss_test_uni, acc_train_uni, acc_test_uni, cnt_uni = results[0]
    print(acc_test_uni, acc_train_uni)
    # loss_train_KL, loss_test_KL, acc_train_KL, acc_test_KL, cnt_KL = results[1]
    # (
    #     loss_train_KLGap,
    #     loss_test_KLGap,
    #     acc_train_KLGap,
    #     acc_test_KLGap,
    #     cnt_KLGap,
    # ) = results[2]

    plt.figure(0)
    plt.xlabel("# Communication rounds", fontsize=16)
    plt.ylabel("Global Loss", fontsize=16)

    plt.plot(
        np.linspace(0, T, num=T), loss_train_uni, "b--", label=r"$P_{uni}$ Train Loss"
    )
    plt.plot(
        np.linspace(0, T, num=T), loss_test_uni, "b-", label=r"$P_{uni}$ Test Loss"
    )
    # plt.plot(
    #     np.linspace(0, T, num=T), loss_train_KL, "r--", label=r"$P_{KL}$ Train Loss"
    # )
    # plt.plot(np.linspace(0, T, num=T), loss_test_KL, "r-", label=r"$P_{KL}$ Test Loss")
    # plt.plot(
    #     np.linspace(0, T, num=T),
    #     loss_train_KLGap,
    #     "m--",
    #     label=r"$P_{KLGap}$ Train Loss",
    # )
    # plt.plot(
    #     np.linspace(0, T, num=T), loss_test_KLGap, "m-", label=r"$P_{KLGap}$ Test Loss"
    # )

    plt.title(r"MNIST $m = 3, M = 10, \alpha = 0.05$")
    plt.legend(loc="lower left")
    plt.show()

    # plt.figure(1)
    # label_list = [
    #     "Uniform Train",
    #     "Uniform Test",
    #     "KL Train",
    #     "KL Test",
    #     "KLGap Train",
    #     "KLGap Test",
    # ]
    # bar_colors = [
    #     "tab:blue",
    #     "tab:orange",
    #     "tab:blue",
    #     "tab:orange",
    #     "tab:blue",
    #     "tab:orange",
    # ]
    # value_list = [
    #     acc_train_uni,
    #     acc_test_uni,
    #     acc_train_KL,
    #     acc_test_KL,
    #     acc_train_KLGap,
    #     acc_test_KLGap,
    # ]

    # plt.title(r"MNIST $m = 3, M = 10, \alpha = 0.05$")
    # plt.bar(label_list, value_list, color=bar_colors, width=0.5)
    # plt.xlabel("Method")
    # plt.ylabel("Accuracy", rotation=0, ha="right")
    # plt.show()


if __name__ == "__main__":
    main()
